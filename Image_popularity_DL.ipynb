{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwAFnsPs2VMR",
        "outputId": "4d1cf3a2-4afb-4133-93b5-ae5516e8ac27"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3k492bhVQeYm",
        "outputId": "9f80d693-3351-4ec9-935f-3c0253b0c24c"
      },
      "source": [
        "%cd '/content/drive/MyDrive/popularity_pred'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/popularity_pred\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkQmwc4VxTho"
      },
      "source": [
        "# !rm -rf models_3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_89qo1jRPqOO"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as XGB\n",
        "from prettytable import PrettyTable\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix, f1_score, silhouette_score\n",
        "\n",
        "from preprocess import standard_scaler\n",
        "from evaluator import spearmanr, reg_eval_metrics\n",
        "from feature_gen import gen_text_features, gen_date_features\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqjNYU2FXaDL"
      },
      "source": [
        "def cluster_data(df, num_clusters, start_day='Day01', end_day='Day30'):\n",
        "    X = df.loc[:, start_day:end_day]\n",
        "    for col in X.columns:\n",
        "        X[col] = X[col]/df['Day30']\n",
        "    X = X.values\n",
        "    #X = np.log(X+ 0.001)\n",
        "\n",
        "    kmeans = KMeans(n_clusters = num_clusters, init = 'k-means++', max_iter =300, n_init = 10, random_state = 0)\n",
        "    kmeans.fit(X)\n",
        "    labels = kmeans.labels_\n",
        "    aucc = silhouette_score(X, labels, metric='euclidean')\n",
        "    print ('wcss: ', kmeans.inertia_, 'silhouette score: ', aucc)\n",
        "\n",
        "    return kmeans, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUTKAlYrQ_4t"
      },
      "source": [
        "def drop_columns(X, cols=['URL', 'Title', 'Description',\n",
        "                            'Tags', 'Country', 'Unnamed: 0_y',\n",
        "                            'Unnamed: 0_x', 'Camera','UserId',\n",
        "                            'Username', 'FlickrId','DatePosted',\n",
        "                            'DateTaken', 'DateCrawl','Latitude', 'Longitude']\n",
        "                ):\n",
        "    for col in cols:\n",
        "        X = X.drop(col, axis=1)\n",
        "    return X\n",
        "\n",
        "def median_rmse(rmse_values):\n",
        "    return np.median(rmse_values)\n",
        "\n",
        "def trmse_median(rmse_values):\n",
        "    rmse_values = np.array(rmse_values)\n",
        "    rmse_values.sort()\n",
        "    q1 = np.percentile(rmse_values, 25)\n",
        "    q3 = np.percentile(rmse_values, 75)\n",
        "    positive_indices = np.where(\n",
        "        (rmse_values>q1) & (rmse_values<q3),\n",
        "        True,\n",
        "        False,\n",
        "    )\n",
        "    t_rmse_values = rmse_values[positive_indices]\n",
        "    return median_rmse(t_rmse_values), t_rmse_values.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2Uiu8V9RM5t",
        "outputId": "27f6273c-cf64-4d2c-8c30-2016042fb813"
      },
      "source": [
        "users_df = pd.read_csv('data/users_TRAIN.csv')\n",
        "image_info_df = pd.read_csv('data/img_info_TRAIN.csv')\n",
        "headers_df = pd.read_csv('data/headers_TRAIN.csv')\n",
        "popularity_df = pd.read_csv('data/popularity_TRAIN.csv')\n",
        "\n",
        "print(users_df.shape, image_info_df.shape, headers_df.shape, popularity_df.shape)\n",
        "\n",
        "cdf = pd.merge(headers_df, image_info_df, on='FlickrId')\n",
        "combined_df = pd.merge(cdf, users_df, on='UserId' )\n",
        "combined_df = pd.merge(combined_df, popularity_df, on='FlickrId')\n",
        "print(f\"Final data: {combined_df.shape}\")\n",
        "\n",
        "combined_df = combined_df.drop_duplicates('URL')\n",
        "print(f\"Dropped final data: {combined_df.shape}\")\n",
        "\n",
        "num_clusters = 2\n",
        "kmeans_period_1, labels_period_1 = cluster_data(combined_df, 3, 'Day01', 'Day10')\n",
        "kmeans_period_2, labels_period_2 = cluster_data(combined_df, 3, 'Day11', 'Day20')\n",
        "kmeans_period_3, labels_period_3 = cluster_data(combined_df, 10, 'Day21', 'Day30')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(23046, 11) (20337, 14) (28383, 7) (20337, 32)\n",
            "Final data: (21950, 61)\n",
            "Dropped final data: (20337, 61)\n",
            "wcss:  4107.478268296869 silhouette score:  0.4067942500651291\n",
            "wcss:  1887.5760776459317 silhouette score:  0.5392313466861997\n",
            "wcss:  236.6461874782201 silhouette score:  0.5715522856718139\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mPV1eWsXw8D",
        "outputId": "ab6968e7-3ea4-4e41-b00b-6f0a9c351c28"
      },
      "source": [
        "X = combined_df.loc[:, 'FlickrId': 'GroupsAvgPictures']\n",
        "\n",
        "X = gen_text_features(X, 'Title')\n",
        "X = gen_date_features(X, 'DatePosted')\n",
        "X = gen_text_features(X, 'Description')\n",
        "\n",
        "X['Tags'] = X['Tags'].map(lambda x: ' '.join(x))\n",
        "X = gen_text_features(X, 'Tags')\n",
        "\n",
        "X = drop_columns(X)\n",
        "\n",
        "X['views_by_contact'] = X['MeanViews'] / (X['Contacts'] + 0.001)\n",
        "X['views_by_num_grps'] = X['MeanViews'] / (X['NumGroups'] + 0.001)\n",
        "X['views_by_photocount'] = X['MeanViews'] / (X['PhotoCount'] + 0.001)\n",
        "X['views_by_grpavg'] = X['MeanViews'] / (X['GroupsAvgPictures'] + 0.001)\n",
        "X['views_by_avg_grp_mem'] = X['MeanViews'] / (X['AvgGroupsMemb'] + 0.001)\n",
        "\n",
        "col_names = X.columns\n",
        "print(col_names)\n",
        "\n",
        "X = standard_scaler(X.values)\n",
        "Y_scale = combined_df['Day30'].values\n",
        "\n",
        "# Y_scale = np.log(Y_scale/30.0 + 0.1)\n",
        "Y_scale = np.log(np.log(Y_scale + 1) + 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['Size', 'NumSets', 'NumGroups', 'AvgGroupsMemb', 'AvgGroupPhotos',\n",
            "       'Ispro', 'HasStats', 'Contacts', 'PhotoCount', 'MeanViews',\n",
            "       'GroupsCount', 'GroupsAvgMembers', 'GroupsAvgPictures',\n",
            "       'Title_word_count', 'Title_num_chars', 'Title_avg_word_len',\n",
            "       'Title_num_uppercase', 'Title_num_title_case', 'DatePosted_year',\n",
            "       'DatePosted_day', 'DatePosted_hour', 'DatePosted_day_of_week',\n",
            "       'Description_word_count', 'Description_num_chars',\n",
            "       'Description_avg_word_len', 'Description_num_uppercase',\n",
            "       'Description_num_title_case', 'Tags_word_count', 'Tags_num_chars',\n",
            "       'Tags_avg_word_len', 'Tags_num_uppercase', 'Tags_num_title_case',\n",
            "       'views_by_contact', 'views_by_num_grps', 'views_by_photocount',\n",
            "       'views_by_grpavg', 'views_by_avg_grp_mem'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwrcvdHmYm0a"
      },
      "source": [
        "def root_mean_squared_error(y_true, y_pred):\n",
        "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
        "\n",
        "class Regressor_RESNET:\n",
        "    def __init__(self, input_shape, feature_maps, file_name, verbose=True, build=True):\n",
        "        if build == True:\n",
        "            self.verbose = verbose\n",
        "            self.model = self.build_model(input_shape, feature_maps, file_name)\n",
        "            if (verbose == True):\n",
        "                self.model.summary()\n",
        "\n",
        "            self.model.save_weights('model_init.hdf5')\n",
        "        return\n",
        "\n",
        "    def build_model(self, input_shape, n_feature_maps, file_name):\n",
        "        # n_feature_maps = 64 * 2\n",
        "\n",
        "        input_layer = keras.layers.Input(input_shape)\n",
        "\n",
        "        # BLOCK 1\n",
        "        conv_x = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=8, padding='same')(input_layer)\n",
        "        conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "        conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "\n",
        "        conv_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=3, padding='same')(conv_x)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "        conv_z = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=3, padding='same')(conv_y)\n",
        "        conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "        # expand channels for the sum\n",
        "        shortcut_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=1, padding='same')(input_layer)\n",
        "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
        "\n",
        "        output_block_1 = keras.layers.add([shortcut_y, conv_z])\n",
        "        output_block_1 = keras.layers.Activation('relu')(output_block_1)\n",
        "\n",
        "        # BLOCK 2\n",
        "        conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(output_block_1)\n",
        "        conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "        conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//16, kernel_size=1, padding='same')(conv_x)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//16, kernel_size=1, padding='same')(conv_y)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "        conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
        "        conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "        # expand channels for the sum\n",
        "        shortcut_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=1, padding='same')(output_block_1)\n",
        "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
        "\n",
        "        output_block_2 = keras.layers.add([shortcut_y, conv_z])\n",
        "        output_block_2 = keras.layers.Activation('relu')(output_block_2)\n",
        "\n",
        "        # BLOCK 3\n",
        "        conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_2)\n",
        "        conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "        conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "\n",
        "        # r= 4 worked better\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//16, kernel_size=1, padding='same')(conv_x)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y_1 = keras.layers.Activation('relu')(conv_y)\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//4, kernel_size=1, padding='same')(conv_y_1)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y_1 = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//16, kernel_size=3, padding='same')(conv_x)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y_2 = keras.layers.Activation('relu')(conv_y)\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//4, kernel_size=3, padding='same')(conv_y_2)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y_2 = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//16, kernel_size=5, padding='same')(conv_x)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y_3 = keras.layers.Activation('relu')(conv_y)\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//4, kernel_size=5, padding='same')(conv_y_3)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y_3 = keras.layers.Activation('relu')(conv_y)\n",
        "        conv_y = keras.layers.add([conv_y_1, conv_y_2, conv_y_3])\n",
        "\n",
        "        conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
        "        conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "        # no need to expand channels because they are equal\n",
        "        shortcut_y = keras.layers.BatchNormalization()(output_block_2)\n",
        "\n",
        "        output_block_3 = keras.layers.add([shortcut_y, conv_z])\n",
        "        output_block_3 = keras.layers.Activation('relu')(output_block_3)\n",
        "\n",
        "        # FINAL\n",
        "        gap_layer = keras.layers.GlobalAveragePooling1D()(output_block_3)\n",
        "        gap_layer = keras.layers.Dropout(0.5)(gap_layer)\n",
        "        gap_layer = keras.layers.Dense(256, activation='relu')(gap_layer)\n",
        "        gap_layer = keras.layers.Dropout(0.5)(gap_layer)\n",
        "        gap_layer = keras.layers.Dense(128, activation='linear')(gap_layer)\n",
        "\n",
        "        output_layer = keras.layers.Dense(1, activation='linear')(gap_layer)\n",
        "\n",
        "        model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "        model.compile(\n",
        "            loss=root_mean_squared_error,\n",
        "            metrics=[tf.keras.metrics.RootMeanSquaredError(name=\"rmse\")],\n",
        "            optimizer=keras.optimizers.Adam()\n",
        "        )\n",
        "\n",
        "        reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=10, min_lr=0.0001)\n",
        "\n",
        "        model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "                              file_name,\n",
        "                              verbose=self.verbose,\n",
        "                              monitor='val_loss',\n",
        "                              save_best_only=True,\n",
        "                              mode='auto'\n",
        "                          )\n",
        "\n",
        "        early_stopping = keras.callbacks.EarlyStopping(\n",
        "                            monitor=\"val_loss\",\n",
        "                            min_delta=0.000001,\n",
        "                            patience=20,\n",
        "                            verbose=0,\n",
        "                            mode=\"auto\",\n",
        "                        )\n",
        "\n",
        "        self.callbacks = [reduce_lr, model_checkpoint, early_stopping]\n",
        "        return model\n",
        "\n",
        "    def fit_predict(self, x_train, y_train, x_val, y_val, file_name, nb_epochs):\n",
        "        if not tf.test.is_gpu_available:\n",
        "            print('error')\n",
        "            exit()\n",
        "\n",
        "        batch_size = 128\n",
        "        mini_batch_size = int(min(x_train.shape[0] / 10, batch_size))\n",
        "\n",
        "        hist = self.model.fit(\n",
        "                    x_train, y_train,\n",
        "                    epochs=nb_epochs,\n",
        "                    verbose=True,\n",
        "                    batch_size=mini_batch_size,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    callbacks=self.callbacks\n",
        "                )\n",
        "\n",
        "        # self.model.save(file_name)\n",
        "        y_pred = None # self.predict(x_val, file_name)\n",
        "        keras.backend.clear_session()\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "    def predict(self, x_test, file_name):\n",
        "        model_path =  file_name\n",
        "        model = keras.models.load_model(model_path, compile=False)\n",
        "        y_pred = model.predict(x_test)\n",
        "\n",
        "        return np.squeeze(y_pred, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDtkC5QzhqDA"
      },
      "source": [
        "class Classifier_RESNET:\n",
        "    def __init__(self, input_shape, feature_maps, file_name, verbose=True, build=True):\n",
        "        if build == True:\n",
        "            self.verbose = verbose\n",
        "            self.model = self.build_model(input_shape, feature_maps, file_name)\n",
        "            if (verbose == True):\n",
        "                self.model.summary()\n",
        "\n",
        "            self.model.save_weights('model_init.hdf5')\n",
        "        return\n",
        "\n",
        "    def build_model(self, input_shape, n_feature_maps, file_name):\n",
        "        # n_feature_maps = 64 * 2\n",
        "\n",
        "        input_layer = keras.layers.Input(input_shape)\n",
        "\n",
        "        # BLOCK 1\n",
        "        conv_x = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=8, padding='same')(input_layer)\n",
        "        conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "        conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "\n",
        "        conv_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=3, padding='same')(conv_x)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "        conv_z = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=3, padding='same')(conv_y)\n",
        "        conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "        # expand channels for the sum\n",
        "        shortcut_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=1, padding='same')(input_layer)\n",
        "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
        "\n",
        "        output_block_1 = keras.layers.add([shortcut_y, conv_z])\n",
        "        output_block_1 = keras.layers.Activation('relu')(output_block_1)\n",
        "\n",
        "        # BLOCK 2\n",
        "        conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(output_block_1)\n",
        "        conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "        conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//16, kernel_size=1, padding='same')(conv_x)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//16, kernel_size=1, padding='same')(conv_y)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "        conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
        "        conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "        # expand channels for the sum\n",
        "        shortcut_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=1, padding='same')(output_block_1)\n",
        "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
        "\n",
        "        output_block_2 = keras.layers.add([shortcut_y, conv_z])\n",
        "        output_block_2 = keras.layers.Activation('relu')(output_block_2)\n",
        "\n",
        "        # BLOCK 3\n",
        "        conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_2)\n",
        "        conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "        conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "\n",
        "        # r= 4 worked better\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//16, kernel_size=1, padding='same')(conv_x)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y_1 = keras.layers.Activation('relu')(conv_y)\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//4, kernel_size=1, padding='same')(conv_y_1)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y_1 = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//16, kernel_size=3, padding='same')(conv_x)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y_2 = keras.layers.Activation('relu')(conv_y)\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//4, kernel_size=3, padding='same')(conv_y_2)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y_2 = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//16, kernel_size=5, padding='same')(conv_x)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y_3 = keras.layers.Activation('relu')(conv_y)\n",
        "        conv_y = keras.layers.Conv1D(filters=(n_feature_maps * 2)//4, kernel_size=5, padding='same')(conv_y_3)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y_3 = keras.layers.Activation('relu')(conv_y)\n",
        "        conv_y = keras.layers.add([conv_y_1, conv_y_2, conv_y_3])\n",
        "\n",
        "        conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
        "        conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "        # no need to expand channels because they are equal\n",
        "        shortcut_y = keras.layers.BatchNormalization()(output_block_2)\n",
        "\n",
        "        output_block_3 = keras.layers.add([shortcut_y, conv_z])\n",
        "        output_block_3 = keras.layers.Activation('relu')(output_block_3)\n",
        "\n",
        "        # FINAL\n",
        "        gap_layer = keras.layers.GlobalAveragePooling1D()(output_block_3)\n",
        "        gap_layer = keras.layers.Dropout(0.5)(gap_layer)\n",
        "        gap_layer = keras.layers.Dense(256, activation='relu')(gap_layer)\n",
        "        gap_layer = keras.layers.Dropout(0.5)(gap_layer)\n",
        "        gap_layer = keras.layers.Dense(128, activation='linear')(gap_layer)\n",
        "        output_layer = keras.layers.Dense(1, activation='sigmoid')(gap_layer)\n",
        "\n",
        "        model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "        model.compile(\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy'],\n",
        "            optimizer=keras.optimizers.Adam()\n",
        "        )\n",
        "\n",
        "        reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=10, min_lr=0.0001)\n",
        "\n",
        "        model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "                              file_name,\n",
        "                              verbose=self.verbose,\n",
        "                              monitor='val_loss',\n",
        "                              save_best_only=True,\n",
        "                              mode='auto'\n",
        "                          )\n",
        "\n",
        "        early_stopping = keras.callbacks.EarlyStopping(\n",
        "                            monitor=\"val_loss\",\n",
        "                            min_delta=0.000001,\n",
        "                            patience=20,\n",
        "                            verbose=0,\n",
        "                            mode=\"auto\",\n",
        "                        )\n",
        "\n",
        "        self.callbacks = [reduce_lr, model_checkpoint, early_stopping]\n",
        "        return model\n",
        "\n",
        "    def fit_predict(self, x_train, y_train, x_val, y_val, file_name, nb_epochs):\n",
        "        if not tf.test.is_gpu_available:\n",
        "            print('error')\n",
        "            exit()\n",
        "\n",
        "        batch_size = 128\n",
        "        mini_batch_size = int(min(x_train.shape[0] / 10, batch_size))\n",
        "\n",
        "        hist = self.model.fit(\n",
        "                    x_train, y_train,\n",
        "                    epochs=nb_epochs,\n",
        "                    verbose=True,\n",
        "                    batch_size=mini_batch_size,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    callbacks=self.callbacks,\n",
        "                )\n",
        "\n",
        "        # self.model.save(file_name)\n",
        "        y_pred = None # self.predict(x_val, file_name)\n",
        "        keras.backend.clear_session()\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "    def predict(self, x_test, file_name):\n",
        "        model_path =  file_name\n",
        "        model = keras.models.load_model(model_path)\n",
        "        y_pred = model.predict(x_test)\n",
        "\n",
        "        y_pred[y_pred > 0.5] = 1\n",
        "        y_pred[y_pred <= 0.5] = 0\n",
        "\n",
        "        return np.squeeze(y_pred, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beoWOAhIYmw6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41deadbb-e34b-4a02-9182-d9842cc6acb7"
      },
      "source": [
        "train_epochs = 500\n",
        "feature_maps = 64\n",
        "\n",
        "feature_maps"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chYsk1MN1ilS"
      },
      "source": [
        "path_to_model = '/content/drive/MyDrive/popularity_pred/models_1'\n",
        "\n",
        "if not os.path.exists(path_to_model):\n",
        "    os.mkdir(path_to_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2T2xd6QYg5F"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3E4tsmXltdU"
      },
      "source": [
        "def classifier(X_train, y_train,\n",
        "                X_test, y_test, col_names=None, period=1, run=1):\n",
        "\n",
        "    # X_train = np.expand_dims(X_train, axis=1)\n",
        "    # X_test = np.expand_dims(X_test, axis=1)\n",
        "\n",
        "    # model_path = f'{path_to_model}/classifier_p{period}_{run}.hdf5'\n",
        "    # clf = Classifier_RESNET(X_train[0].shape, feature_maps, model_path, verbose=False)\n",
        "\n",
        "    # if col_names is not None:\n",
        "    #     X_train = pd.DataFrame(data=X_train, columns=col_names)\n",
        "    #     X_test = pd.DataFrame(data=X_test, columns=col_names)\n",
        "\n",
        "    # if run == 3:\n",
        "    # if not os.path.isfile(model_path):\n",
        "    #     clf.fit_predict(\n",
        "    #         X_train, y_train, X_test, y_test,\n",
        "    #         model_path, train_epochs\n",
        "    #     )\n",
        "    clf = RandomForestClassifier()\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_train_pred = clf.predict(X_train)\n",
        "\n",
        "    # print(y_train_pred.shape, y_train.shape)\n",
        "    fold_score = f1_score(y_train, y_train_pred, average='weighted')\n",
        "    print (\"train data f1 score: \", fold_score)\n",
        "\n",
        "    y_pred = clf.predict(X_test)\n",
        "    fold_score = f1_score(y_test, y_pred, average='weighted')\n",
        "    print (\"classifier f1 score: \", fold_score)\n",
        "\n",
        "    return y_pred, fold_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4l9mU10pltay"
      },
      "source": [
        "def regressor(X_train_folds_reg, y_train_folds_reg,\n",
        "              X_test_fold_reg, y_test_fold_reg, col_names=None, period=1, run=1):\n",
        "\n",
        "    # X_train_folds_reg = np.expand_dims(X_train_folds_reg, axis=1)\n",
        "    # X_test_fold_reg = np.expand_dims(X_test_fold_reg, axis=1)\n",
        "\n",
        "    # model_path = f'{path_to_model}/regressor_p{period}_{run}.hdf5'\n",
        "    # clf = Regressor_RESNET(X_train_folds_reg[0].shape, feature_maps, model_path, verbose=False)\n",
        "\n",
        "    # if col_names is not None:\n",
        "    #     X_train_folds_reg = pd.DataFrame(data=X_train_folds_reg, columns=col_names)\n",
        "    #     X_test_fold_reg = pd.DataFrame(data=X_test_fold_reg, columns=col_names)\n",
        "\n",
        "    # if not os.path.isfile(model_path):\n",
        "    #     clf.fit_predict(\n",
        "    #         X_train_folds_reg, y_train_folds_reg,\n",
        "    #         X_test_fold_reg, y_test_fold_reg,\n",
        "    #         model_path, train_epochs\n",
        "    #     )\n",
        "\n",
        "    # clf = None\n",
        "    clf = RandomForestRegressor()\n",
        "    clf.fit(X_train_folds_reg, y_train_folds_reg)\n",
        "\n",
        "    # ypred = clf.predict(X_train_folds_reg, model_path)\n",
        "    ypred = clf.predict(X_train_folds_reg)\n",
        "    # print(y_train_folds_reg.shape, ypred.shape)\n",
        "    (rmse, mae, r2) = reg_eval_metrics(y_train_folds_reg, ypred)\n",
        "    p = PrettyTable(['RMSE', 'MAE', 'R2', 'Spearmanr'])\n",
        "    p.add_row([rmse, mae, r2, spearmanr(y_train_folds_reg, ypred)])\n",
        "    print(p)\n",
        "\n",
        "    # ypred = clf.predict(X_test_fold_reg, model_path)\n",
        "    ypred = clf.predict(X_test_fold_reg)\n",
        "    (rmse, mae, r2) = reg_eval_metrics(y_test_fold_reg, ypred)\n",
        "    p = PrettyTable(['RMSE', 'MAE', 'R2', 'Spearmanr'])\n",
        "    p.add_row([rmse, mae, r2, spearmanr(y_test_fold_reg, ypred)])\n",
        "    print(p)\n",
        "\n",
        "    return ypred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSDagXA4tlWQ"
      },
      "source": [
        "def overall_eval_metric(cluster_org_labels, cluster_centers, gt_scale, final_gt,\n",
        "                        double_log_trans=True):\n",
        "\n",
        "    missed = 0\n",
        "    total_rmse, total_mae, total_r2, corr = 0, 0, 0, 0\n",
        "    truncated_rmse, truncated_mae = [], []\n",
        "\n",
        "    for i in range(len(gt_scale)):\n",
        "        #pred = cluster_centers[cluster_org_labels[i]]* ((np.exp(gt_scale[i]) -1)*30)\n",
        "        # applied log transform now redo the operation\n",
        "        if double_log_trans:\n",
        "            pred = cluster_centers[int(cluster_org_labels[i])] * (np.exp(np.exp(gt_scale[i])-1)-1)\n",
        "        else:\n",
        "            pred = cluster_centers[cluster_org_labels[i]] * (np.exp(gt_scale[i])-1) * 30\n",
        "        gt = final_gt[i]\n",
        "        s_corr = spearmanr(gt, pred)\n",
        "        rmse, mae, r2 = reg_eval_metrics(gt, pred)\n",
        "        truncated_rmse.append(rmse)\n",
        "        truncated_mae.append(mae)\n",
        "\n",
        "        if not np.isnan(s_corr):\n",
        "            corr += s_corr\n",
        "            total_rmse += rmse\n",
        "            total_mae += mae\n",
        "            total_r2 += r2\n",
        "        else:\n",
        "            missed+=1\n",
        "\n",
        "    median_trmse, mean_trmse = trmse_median(truncated_rmse)\n",
        "    median_tmae, mean_tmae = trmse_median(truncated_mae)\n",
        "\n",
        "    return median_trmse, mean_trmse, median_tmae, mean_tmae"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYxwcktVltXt"
      },
      "source": [
        "def get_results(X, Y, Y_scale, kmeans, train_index, test_index, start_day, end_day, col_names=None, scale_pos_weight=1, period=1, run=1):\n",
        "    # X = np.expand_dims(X, axis=1)\n",
        "    print(X.shape)\n",
        "\n",
        "    X_train_fold = X[train_index]\n",
        "    y_train_fold = Y[train_index]\n",
        "    X_test_fold = X[test_index]\n",
        "    y_test_fold = Y[test_index]\n",
        "\n",
        "    y_pred, fold_score = classifier(X_train_fold, y_train_fold,\n",
        "                                    X_test_fold, y_test_fold, col_names, period, run)\n",
        "    print (confusion_matrix(y_test_fold, y_pred))\n",
        "\n",
        "    # Scale Reg\n",
        "    y_train_reg = Y_scale[train_index]\n",
        "    y_test_reg  = Y_scale[test_index]\n",
        "    scale_pred  = regressor(X_train_fold, y_train_reg,\n",
        "                            X_test_fold, y_test_reg, col_names, period, run)\n",
        "\n",
        "    corr = 0\n",
        "    gt_ = combined_df.loc[:, start_day:end_day].values[test_index]\n",
        "\n",
        "    casea_median, casea_mean, casea_mae_median, casea_mae_mean = overall_eval_metric(cluster_org_labels=Y[test_index],\n",
        "            cluster_centers = kmeans.cluster_centers_ ,\n",
        "            gt_scale=y_test_reg,\n",
        "            final_gt=gt_)\n",
        "\n",
        "    caseb_median, caseb_mean, caseb_mae_median, caseb_mae_mean = overall_eval_metric(cluster_org_labels=y_pred,\n",
        "            cluster_centers = kmeans.cluster_centers_,\n",
        "            gt_scale=y_test_reg,\n",
        "            final_gt=gt_)\n",
        "\n",
        "    casec_median, casec_mean, casec_mae_median, casec_mae_mean = overall_eval_metric(cluster_org_labels=Y[test_index],\n",
        "            cluster_centers = kmeans.cluster_centers_ ,\n",
        "            gt_scale=scale_pred,\n",
        "            final_gt=gt_)\n",
        "\n",
        "    cased_median, cased_mean, cased_mae_median, cased_mae_mean = overall_eval_metric(cluster_org_labels=y_pred,\n",
        "            cluster_centers = kmeans.cluster_centers_,\n",
        "            gt_scale=scale_pred,\n",
        "            final_gt=gt_)\n",
        "    return casea_median, casea_mean, caseb_median, caseb_mean, casec_median, casec_mean, cased_median, cased_mean, casea_mae_median, casea_mae_mean, caseb_mae_median, caseb_mae_mean, casec_mae_median, casec_mae_mean, cased_mae_median, cased_mae_mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvMnKLOtYmty"
      },
      "source": [
        "p1a_avg_mean, p1a_avg_median = [], []\n",
        "p2a_avg_mean, p2a_avg_median = [], []\n",
        "p3a_avg_mean, p3a_avg_median = [], []\n",
        "\n",
        "p1b_avg_mean, p1b_avg_median = [], []\n",
        "p2b_avg_mean, p2b_avg_median = [], []\n",
        "p3b_avg_mean, p3b_avg_median = [], []\n",
        "\n",
        "p1c_avg_mean, p1c_avg_median = [], []\n",
        "p2c_avg_mean, p2c_avg_median = [], []\n",
        "p3c_avg_mean, p3c_avg_median = [], []\n",
        "\n",
        "p1d_avg_mean, p1d_avg_median = [], []\n",
        "p2d_avg_mean, p2d_avg_median = [], []\n",
        "p3d_avg_mean, p3d_avg_median = [], []\n",
        "\n",
        "##############################################################\n",
        "\n",
        "p1a_mae_avg_mean, p1a_mae_avg_median = [], []\n",
        "p2a_mae_avg_mean, p2a_mae_avg_median = [], []\n",
        "p3a_mae_avg_mean, p3a_mae_avg_median = [], []\n",
        "\n",
        "p1b_mae_avg_mean, p1b_mae_avg_median = [], []\n",
        "p2b_mae_avg_mean, p2b_mae_avg_median = [], []\n",
        "p3b_mae_avg_mean, p3b_mae_avg_median = [], []\n",
        "\n",
        "p1c_mae_avg_mean, p1c_mae_avg_median = [], []\n",
        "p2c_mae_avg_mean, p2c_mae_avg_median = [], []\n",
        "p3c_mae_avg_mean, p3c_mae_avg_median = [], []\n",
        "\n",
        "p1d_mae_avg_mean, p1d_mae_avg_median = [], []\n",
        "p2d_mae_avg_mean, p2d_mae_avg_median = [], []\n",
        "p3d_mae_avg_mean, p3d_mae_avg_median = [], []\n",
        "\n",
        "##############################################################\n",
        "\n",
        "tablea = PrettyTable([\"p1_mean\", \"p2_mean\", \"p3_mean\", \"p1_median\", \"p2_median\", \"p3_median\"])\n",
        "tableb = PrettyTable([\"p1_mean\", \"p2_mean\", \"p3_mean\", \"p1_median\", \"p2_median\", \"p3_median\"])\n",
        "tablec = PrettyTable([\"p1_mean\", \"p2_mean\", \"p3_mean\", \"p1_median\", \"p2_median\", \"p3_median\"])\n",
        "tabled = PrettyTable([\"p1_mean\", \"p2_mean\", \"p3_mean\", \"p1_median\", \"p2_median\", \"p3_median\"])\n",
        "\n",
        "##############################################################\n",
        "\n",
        "tablea_mae = PrettyTable([\"p1_mae_mean\", \"p2_mae_mean\", \"p3_mae_mean\", \"p1_mae_median\", \"p2_mae_median\", \"p3_mae_median\"])\n",
        "tableb_mae = PrettyTable([\"p1_mae_mean\", \"p2_mae_mean\", \"p3_mae_mean\", \"p1_mae_median\", \"p2_mae_median\", \"p3_mae_median\"])\n",
        "tablec_mae = PrettyTable([\"p1_mae_mean\", \"p2_mae_mean\", \"p3_mae_mean\", \"p1_mae_median\", \"p2_mae_median\", \"p3_mae_median\"])\n",
        "tabled_mae = PrettyTable([\"p1_mae_mean\", \"p2_mae_mean\", \"p3_mae_mean\", \"p1_mae_median\", \"p2_mae_median\", \"p3_mae_median\"])\n",
        "\n",
        "##############################################################\n",
        "\n",
        "n_splits = 3\n",
        "skfolds = StratifiedKFold(n_splits=n_splits, random_state=42, shuffle=True)\n",
        "\n",
        "period_1_test_indices = []\n",
        "period_1_train_indices = []\n",
        "for train_index, test_index in skfolds.split(X, labels_period_1):\n",
        "    period_1_train_indices.append(train_index)\n",
        "    period_1_test_indices.append(test_index)\n",
        "\n",
        "period_2_test_indices = []\n",
        "period_2_train_indices = []\n",
        "for train_index, test_index in skfolds.split(X, labels_period_2):\n",
        "    period_2_train_indices.append(train_index)\n",
        "    period_2_test_indices.append(test_index)\n",
        "\n",
        "period_3_test_indices = []\n",
        "period_3_train_indices = []\n",
        "for train_index, test_index in skfolds.split(X, labels_period_3):\n",
        "    period_3_train_indices.append(train_index)\n",
        "    period_3_test_indices.append(test_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WWWGrDTFun-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "outputId": "a8c3fae5-4326-4380-f40e-818e493c089c"
      },
      "source": [
        "n = 10\n",
        "\n",
        "for i in range(n_splits):\n",
        "    print (\"&&&&\"*n, f\"Run {i+1} Period1 \", \"&&&&\"*n)\n",
        "    p1a_median, p1a_mean, p1b_median, p1b_mean, p1c_median, p1c_mean, p1d_median, p1d_mean, p1a_mae_median, p1a_mae_mean, p1b_mae_median, p1b_mae_mean, p1c_mae_median, p1c_mae_mean, p1d_mae_median, p1d_mae_mean \\\n",
        "      = get_results(X, labels_period_1, Y_scale, kmeans_period_1, period_1_train_indices[i], period_1_test_indices[i], 'Day01', 'Day10', col_names, 1, period=1, run=i+1)\n",
        "    print (\"&&&&\"*n, f\"Run {i+1} Period2 \", \"&&&&\"*n)\n",
        "    p2a_median, p2a_mean, p2b_median, p2b_mean, p2c_median, p2c_mean, p2d_median, p2d_mean, p2a_mae_median, p2a_mae_mean, p2b_mae_median, p2b_mae_mean, p2c_mae_median, p2c_mae_mean, p2d_mae_median, p2d_mae_mean \\\n",
        "      = get_results(X, labels_period_2, Y_scale, kmeans_period_2, period_2_train_indices[i], period_2_test_indices[i], 'Day11', 'Day20', col_names, 1, period=2, run=i+1)\n",
        "    print (\"&&&&\"*n, f\"Run {i+1} Period3 \", \"&&&&\"*n)\n",
        "    p3a_median, p3a_mean, p3b_median, p3b_mean, p3c_median, p3c_mean, p3d_median, p3d_mean, p3a_mae_median, p3a_mae_mean, p3b_mae_median, p3b_mae_mean, p3c_mae_median, p3c_mae_mean, p3d_mae_median, p3d_mae_mean \\\n",
        "      = get_results(X, labels_period_3, Y_scale, kmeans_period_3, period_3_train_indices[i], period_3_test_indices[i], 'Day21', 'Day30', col_names, 1, period=3, run=i+1)\n",
        "\n",
        "\n",
        "    tablea.add_row([p1a_mean, p2a_mean, p3a_mean, p1a_median, p2a_median, p3a_median])\n",
        "    tableb.add_row([p1b_mean, p2b_mean, p3b_mean, p1b_median, p2b_median, p3b_median])\n",
        "    tablec.add_row([p1c_mean, p2c_mean, p3c_mean, p1c_median, p2c_median, p3c_median])\n",
        "    tabled.add_row([p1d_mean, p2d_mean, p3d_mean, p1d_median, p2d_median, p3d_median])\n",
        "\n",
        "\n",
        "    p1a_avg_mean.append(p1a_mean)\n",
        "    p1a_avg_median.append(p1a_median)\n",
        "    p2a_avg_mean.append(p2a_mean)\n",
        "    p2a_avg_median.append(p2a_median)\n",
        "    p3a_avg_mean.append(p3a_mean)\n",
        "    p3a_avg_median.append(p3a_median)\n",
        "\n",
        "    p1b_avg_mean.append(p1b_mean)\n",
        "    p1b_avg_median.append(p1b_median)\n",
        "    p2b_avg_mean.append(p2b_mean)\n",
        "    p2b_avg_median.append(p2b_median)\n",
        "    p3b_avg_mean.append(p3b_mean)\n",
        "    p3b_avg_median.append(p3b_median)\n",
        "\n",
        "    p1c_avg_mean.append(p1c_mean)\n",
        "    p1c_avg_median.append(p1c_median)\n",
        "    p2c_avg_mean.append(p2c_mean)\n",
        "    p2c_avg_median.append(p2c_median)\n",
        "    p3c_avg_mean.append(p3c_mean)\n",
        "    p3c_avg_median.append(p3c_median)\n",
        "\n",
        "    p1d_avg_mean.append(p1d_mean)\n",
        "    p1d_avg_median.append(p1d_median)\n",
        "    p2d_avg_mean.append(p2d_mean)\n",
        "    p2d_avg_median.append(p2d_median)\n",
        "    p3d_avg_mean.append(p3d_mean)\n",
        "    p3d_avg_median.append(p3d_median)\n",
        "\n",
        "    ############################################################################\n",
        "\n",
        "    tablea_mae.add_row([p1a_mae_mean, p2a_mae_mean, p3a_mae_mean, p1a_mae_median, p2a_mae_median, p3a_mae_median])\n",
        "    tableb_mae.add_row([p1b_mae_mean, p2b_mae_mean, p3b_mae_mean, p1b_mae_median, p2b_mae_median, p3b_mae_median])\n",
        "    tablec_mae.add_row([p1c_mae_mean, p2c_mae_mean, p3c_mae_mean, p1c_mae_median, p2c_mae_median, p3c_mae_median])\n",
        "    tabled_mae.add_row([p1d_mae_mean, p2d_mae_mean, p3d_mae_mean, p1d_mae_median, p2d_mae_median, p3d_mae_median])\n",
        "\n",
        "\n",
        "    p1a_mae_avg_mean.append(p1a_mae_mean)\n",
        "    p1a_mae_avg_median.append(p1a_mae_median)\n",
        "    p2a_mae_avg_mean.append(p2a_mae_mean)\n",
        "    p2a_mae_avg_median.append(p2a_mae_median)\n",
        "    p3a_mae_avg_mean.append(p3a_mae_mean)\n",
        "    p3a_mae_avg_median.append(p3a_mae_median)\n",
        "\n",
        "    p1b_mae_avg_mean.append(p1b_mae_mean)\n",
        "    p1b_mae_avg_median.append(p1b_mae_median)\n",
        "    p2b_mae_avg_mean.append(p2b_mae_mean)\n",
        "    p2b_mae_avg_median.append(p2b_mae_median)\n",
        "    p3b_mae_avg_mean.append(p3b_mae_mean)\n",
        "    p3b_mae_avg_median.append(p3b_mae_median)\n",
        "\n",
        "    p1c_mae_avg_mean.append(p1c_mae_mean)\n",
        "    p1c_mae_avg_median.append(p1c_mae_median)\n",
        "    p2c_mae_avg_mean.append(p2c_mae_mean)\n",
        "    p2c_mae_avg_median.append(p2c_mae_median)\n",
        "    p3c_mae_avg_mean.append(p3c_mae_mean)\n",
        "    p3c_mae_avg_median.append(p3c_mae_median)\n",
        "\n",
        "    p1d_mae_avg_mean.append(p1d_mae_mean)\n",
        "    p1d_mae_avg_median.append(p1d_mae_median)\n",
        "    p2d_mae_avg_mean.append(p2d_mae_mean)\n",
        "    p2d_mae_avg_median.append(p2d_mae_median)\n",
        "    p3d_mae_avg_mean.append(p3d_mae_mean)\n",
        "    p3d_mae_avg_median.append(p3d_mae_median)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&& Run 1 Period1  &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
            "(20337, 37)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-e956edfbdaa6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"&&&&\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Run {i+1} Period1 \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"&&&&\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mp1a_median\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp1a_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp1b_median\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp1b_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp1c_median\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp1c_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp1d_median\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp1d_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp1a_mae_median\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp1a_mae_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp1b_mae_median\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp1b_mae_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp1c_mae_median\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp1c_mae_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp1d_mae_median\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp1d_mae_mean\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0mget_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_period_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkmeans_period_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiod_1_train_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiod_1_test_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Day01'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Day10'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"&&&&\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Run {i+1} Period2 \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"&&&&\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mp2a_median\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2a_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2b_median\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2b_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2c_median\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2c_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2d_median\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2d_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2a_mae_median\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2a_mae_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2b_mae_median\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2b_mae_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2c_mae_median\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2c_mae_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2d_mae_median\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2d_mae_mean\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0mget_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_period_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkmeans_period_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiod_2_train_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiod_2_test_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Day11'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Day20'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-8a7ef6055a9b>\u001b[0m in \u001b[0;36mget_results\u001b[0;34m(X, Y, Y_scale, kmeans, train_index, test_index, start_day, end_day, col_names, scale_pos_weight, period, run)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     y_pred, fold_score = classifier(X_train_fold, y_train_fold,\n\u001b[0;32m---> 11\u001b[0;31m                                     X_test_fold, y_test_fold, col_names, period, run)\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-f8caa7a3a037>\u001b[0m in \u001b[0;36mclassifier\u001b[0;34m(X_train, y_train, X_test, y_test, col_names, period, run)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#     )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0my_train_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    381\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                     n_samples_bootstrap=n_samples_bootstrap)\n\u001b[0;32m--> 383\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    878\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    365\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2zpBJA32S-C"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCOjA14YtKap",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4cb9c92-90e5-4f57-eb9f-8106c80da0fa"
      },
      "source": [
        "print(tablea)\n",
        "print(tableb)\n",
        "print(tablec)\n",
        "print(tabled)\n",
        "\n",
        "# print(tablea_mae)\n",
        "# print(tableb_mae)\n",
        "# print(tablec_mae)\n",
        "# print(tabled_mae)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+---------------------+--------------------+--------------------+--------------------+\n",
            "|      p1_mean       |      p2_mean       |       p3_mean       |     p1_median      |     p2_median      |     p3_median      |\n",
            "+--------------------+--------------------+---------------------+--------------------+--------------------+--------------------+\n",
            "| 2.062178944722352  | 1.2527595519956747 |  0.3807142487857374 | 1.5356280371879159 | 0.9447758342698133 | 0.3224860062204702 |\n",
            "| 2.156862959483312  | 1.2021750869841272 | 0.36206910751740545 | 1.6137378508370015 | 0.893397389592911  | 0.3008840576225813 |\n",
            "| 2.1641869709129735 | 1.244868173890592  | 0.37200296974484476 | 1.6488597278905823 | 0.9434717707979464 | 0.2994800421866507 |\n",
            "+--------------------+--------------------+---------------------+--------------------+--------------------+--------------------+\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|      p1_mean      |      p2_mean       |      p3_mean       |     p1_median      |     p2_median      |     p3_median      |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "| 3.392357987929177 | 2.020400907429201  | 0.7179693529767446 | 2.608041600472926  | 1.4981177726677317 | 0.5939795961793483 |\n",
            "| 3.482775283979419 | 2.0079839656120297 | 0.711240964199269  | 2.605981982692962  | 1.5328552854306066 |  0.58824322505333  |\n",
            "|  3.49518444782387 | 2.030447556007805  | 0.7247993251181838 | 2.6646483057040506 | 1.5098398263915211 | 0.6068060050161325 |\n",
            "+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "+-------------------+-------------------+--------------------+-------------------+-------------------+-------------------+\n",
            "|      p1_mean      |      p2_mean      |      p3_mean       |     p1_median     |     p2_median     |     p3_median     |\n",
            "+-------------------+-------------------+--------------------+-------------------+-------------------+-------------------+\n",
            "| 6.527641323927678 |  9.10224359006555 | 10.406331997615139 | 4.728909241943807 | 6.288284137961984 | 7.075994795570835 |\n",
            "| 7.102455605834939 | 8.796096701122439 | 10.112921228757536 | 4.995004823794803 | 6.089691112202457 | 6.868912045004174 |\n",
            "| 7.096887703322626 | 9.076774906991119 | 10.153407928588779 | 4.818530366421471 | 6.241252342173695 | 7.029012980005599 |\n",
            "+-------------------+-------------------+--------------------+-------------------+-------------------+-------------------+\n",
            "+-------------------+-------------------+--------------------+--------------------+-------------------+-------------------+\n",
            "|      p1_mean      |      p2_mean      |      p3_mean       |     p1_median      |     p2_median     |     p3_median     |\n",
            "+-------------------+-------------------+--------------------+--------------------+-------------------+-------------------+\n",
            "| 6.892079868360105 | 9.508880006618691 | 10.540538438200256 | 5.0256980244214144 | 6.706266745939949 | 7.250027843059541 |\n",
            "| 7.501704990196728 | 9.186086354838952 | 10.228789704375563 | 5.355182998713122  | 6.505212782011285 | 7.077681270445858 |\n",
            "| 7.451367876377679 | 9.476777089981429 | 10.30565573136395  | 5.156329761054172  | 6.698856015039934 |  7.24056513232594 |\n",
            "+-------------------+-------------------+--------------------+--------------------+-------------------+-------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InhkLfslFtKZ"
      },
      "source": [
        "def get_std(a, b, c):\n",
        "    # mean = np.mean([np.mean(a), np.mean(b), np.mean(c)])\n",
        "    # std = np.std([np.mean(a), np.mean(b), np.mean(c)])\n",
        "\n",
        "    # a1, b1, c1 = [], [], []\n",
        "    each_mean = []\n",
        "    for i, j, k in zip(a, b, c):\n",
        "        each_mean.append((i+j+k) / 3)\n",
        "\n",
        "    return np.mean(each_mean), np.std(each_mean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgUkQIzJHkdW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2e33af4-d05a-483f-fc24-2e9529cab75e"
      },
      "source": [
        "print(\"CASE A\")\n",
        "table = PrettyTable([\"cluster\", \"p1_avg_mean\", \"p2_avg_mean\", \"p3_avg_mean\", \"p1_median\", \"p2_median\", \"p3_median\", \"Final_mean\", \"Final_median\"])\n",
        "\n",
        "p1_avg_mean = p1a_avg_mean\n",
        "p2_avg_mean = p2a_avg_mean\n",
        "p3_avg_mean = p3a_avg_mean\n",
        "\n",
        "p1_avg_median = p1a_avg_median\n",
        "p2_avg_median = p2a_avg_median\n",
        "p3_avg_median = p3a_avg_median\n",
        "\n",
        "f_median, f_median_std = get_std(p1_avg_median, p2_avg_median, p3_avg_median)\n",
        "f_mean, f_std = get_std(p1_avg_mean, p2_avg_mean, p3_avg_mean)\n",
        "\n",
        "table.add_row([num_clusters, round(np.mean(p1_avg_mean), 3), round(np.mean(p2_avg_mean), 3), np.mean(p3_avg_mean),\n",
        "                np.mean(p1_avg_median), np.mean(p2_avg_median), np.mean(p3_avg_median),\n",
        "                f\"{round(f_mean, 3)} ± {round(f_std, 3)}\", f\"{round(f_median, 3)} ± {round(f_median_std, 3)}\"])\n",
        "print(table)\n",
        "\n",
        "print(\"CASE B\")\n",
        "table = PrettyTable([\"cluster\", \"p1_avg_mean\", \"p2_avg_mean\", \"p3_avg_mean\", \"p1_median\", \"p2_median\", \"p3_median\", \"Final_mean\", \"Final_median\"])\n",
        "\n",
        "p1_avg_mean = p1b_avg_mean\n",
        "p2_avg_mean = p2b_avg_mean\n",
        "p3_avg_mean = p3b_avg_mean\n",
        "\n",
        "p1_avg_median = p1b_avg_median\n",
        "p2_avg_median = p2b_avg_median\n",
        "p3_avg_median = p3b_avg_median\n",
        "\n",
        "f_mean, f_std = get_std(p1_avg_mean, p2_avg_mean, p3_avg_mean)\n",
        "f_median, f_median_std = get_std(p1_avg_median, p2_avg_median, p3_avg_median)\n",
        "\n",
        "table.add_row([num_clusters, round(np.mean(p1_avg_mean), 3), round(np.mean(p2_avg_mean), 3), np.mean(p3_avg_mean),\n",
        "                np.mean(p1_avg_median), np.mean(p2_avg_median), np.mean(p3_avg_median),\n",
        "                f\"{round(f_mean, 3)} ± {round(f_std, 3)}\", f\"{round(f_median, 3)} ± {round(f_median_std, 3)}\"])\n",
        "print(table)\n",
        "\n",
        "print(\"CASE C\")\n",
        "table = PrettyTable([\"cluster\", \"p1_avg_mean\", \"p2_avg_mean\", \"p3_avg_mean\", \"p1_median\", \"p2_median\", \"p3_median\", \"Final_mean\", \"Final_median\"])\n",
        "\n",
        "p1_avg_mean = p1c_avg_mean\n",
        "p2_avg_mean = p2c_avg_mean\n",
        "p3_avg_mean = p3c_avg_mean\n",
        "\n",
        "p1_avg_median = p1c_avg_median\n",
        "p2_avg_median = p2c_avg_median\n",
        "p3_avg_median = p3c_avg_median\n",
        "\n",
        "f_mean, f_std = get_std(p1_avg_mean, p2_avg_mean, p3_avg_mean)\n",
        "f_median, f_median_std = get_std(p1_avg_median, p2_avg_median, p3_avg_median)\n",
        "\n",
        "table.add_row([num_clusters, round(np.mean(p1_avg_mean), 3), round(np.mean(p2_avg_mean), 3), np.mean(p3_avg_mean),\n",
        "                np.mean(p1_avg_median), np.mean(p2_avg_median), np.mean(p3_avg_median),\n",
        "                f\"{round(f_mean, 3)} ± {round(f_std, 3)}\", f\"{round(f_median, 3)} ± {round(f_median_std, 3)}\"])\n",
        "print(table)\n",
        "\n",
        "print(\"CASE D\")\n",
        "table = PrettyTable([\"cluster\", \"p1_avg_mean\", \"p2_avg_mean\", \"p3_avg_mean\", \"p1_median\", \"p2_median\", \"p3_median\", \"Final_mean\", \"Final_median\"])\n",
        "\n",
        "p1_avg_mean = p1d_avg_mean\n",
        "p2_avg_mean = p2d_avg_mean\n",
        "p3_avg_mean = p3d_avg_mean\n",
        "\n",
        "p1_avg_median = p1d_avg_median\n",
        "p2_avg_median = p2d_avg_median\n",
        "p3_avg_median = p3d_avg_median\n",
        "\n",
        "f_mean, f_std = get_std(p1_avg_mean, p2_avg_mean, p3_avg_mean)\n",
        "f_median, f_median_std = get_std(p1_avg_median, p2_avg_median, p3_avg_median)\n",
        "\n",
        "table.add_row([num_clusters, round(np.mean(p1_avg_mean), 3), round(np.mean(p2_avg_mean), 3), np.mean(p3_avg_mean),\n",
        "                np.mean(p1_avg_median), np.mean(p2_avg_median), np.mean(p3_avg_median),\n",
        "                f\"{round(f_mean, 3)} ± {round(f_std, 3)}\", f\"{round(f_median, 3)} ± {round(f_median_std, 3)}\"])\n",
        "print(table)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CASE A\n",
            "+---------+-------------+-------------+---------------------+--------------------+--------------------+--------------------+---------------+---------------+\n",
            "| cluster | p1_avg_mean | p2_avg_mean |     p3_avg_mean     |     p1_median      |     p2_median      |     p3_median      |   Final_mean  |  Final_median |\n",
            "+---------+-------------+-------------+---------------------+--------------------+--------------------+--------------------+---------------+---------------+\n",
            "|    2    |    2.128    |    1.233    | 0.37159544201599587 | 1.5994085386384997 | 0.9272149982202236 | 0.3076167020099007 | 1.244 ± 0.012 | 0.945 ± 0.014 |\n",
            "+---------+-------------+-------------+---------------------+--------------------+--------------------+--------------------+---------------+---------------+\n",
            "CASE B\n",
            "+---------+-------------+-------------+--------------------+-------------------+-------------------+--------------------+---------------+---------------+\n",
            "| cluster | p1_avg_mean | p2_avg_mean |    p3_avg_mean     |     p1_median     |     p2_median     |     p3_median      |   Final_mean  |  Final_median |\n",
            "+---------+-------------+-------------+--------------------+-------------------+-------------------+--------------------+---------------+---------------+\n",
            "|    2    |    3.457    |     2.02    | 0.7180032140980658 | 2.626223962956646 | 1.513604294829953 | 0.5963429420829369 | 2.065 ± 0.016 | 1.579 ± 0.011 |\n",
            "+---------+-------------+-------------+--------------------+-------------------+-------------------+--------------------+---------------+---------------+\n",
            "CASE C\n",
            "+---------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+---------------+---------------+\n",
            "| cluster | p1_avg_mean | p2_avg_mean |    p3_avg_mean    |     p1_median     |     p2_median     |     p3_median     |   Final_mean  |  Final_median |\n",
            "+---------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+---------------+---------------+\n",
            "|    2    |    6.909    |    8.992    | 10.22422038498715 | 4.847481477386694 | 6.206409197446045 | 6.991306606860203 | 8.708 ± 0.048 | 6.015 ± 0.022 |\n",
            "+---------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+---------------+---------------+\n",
            "CASE D\n",
            "+---------+-------------+-------------+--------------------+--------------------+-------------------+-------------------+--------------+---------------+\n",
            "| cluster | p1_avg_mean | p2_avg_mean |    p3_avg_mean     |     p1_median      |     p2_median     |     p3_median     |  Final_mean  |  Final_median |\n",
            "+---------+-------------+-------------+--------------------+--------------------+-------------------+-------------------+--------------+---------------+\n",
            "|    2    |    7.282    |    9.391    | 10.358327957979924 | 5.1790702613962365 | 6.636778514330389 | 7.189424748610446 | 9.01 ± 0.048 | 6.335 ± 0.022 |\n",
            "+---------+-------------+-------------+--------------------+--------------------+-------------------+-------------------+--------------+---------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y32r98l4ysQV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e0ccaec-64f7-41fe-b71b-48ff0d5e2fae"
      },
      "source": [
        "### MAE\n",
        "\n",
        "print(\"CASE A\")\n",
        "table = PrettyTable([\"cluster\", \"p1_avg_mean\", \"p2_avg_mean\", \"p3_avg_mean\", \"p1_median\", \"p2_median\", \"p3_median\", \"Final_mean\", \"Final_median\"])\n",
        "\n",
        "p1_avg_mean = p1a_mae_avg_mean\n",
        "p2_avg_mean = p2a_mae_avg_mean\n",
        "p3_avg_mean = p3a_mae_avg_mean\n",
        "\n",
        "p1_avg_median = p1a_mae_avg_median\n",
        "p2_avg_median = p2a_mae_avg_median\n",
        "p3_avg_median = p3a_mae_avg_median\n",
        "\n",
        "f_mean, f_std = get_std(p1_avg_mean, p2_avg_mean, p3_avg_mean)\n",
        "f_median, f_median_std = get_std(p1_avg_median, p2_avg_median, p3_avg_median)\n",
        "\n",
        "table.add_row([num_clusters, round(np.mean(p1_avg_mean), 3), round(np.mean(p2_avg_mean), 3), np.mean(p3_avg_mean),\n",
        "                np.mean(p1_avg_median), np.mean(p2_avg_median), np.mean(p3_avg_median),\n",
        "                f\"{round(f_mean, 3)} ± {round(f_std, 3)}\", f\"{round(f_median, 3)} ± {round(f_median_std, 3)}\"])\n",
        "print(table)\n",
        "\n",
        "print(\"CASE B\")\n",
        "table = PrettyTable([\"cluster\", \"p1_avg_mean\", \"p2_avg_mean\", \"p3_avg_mean\", \"p1_median\", \"p2_median\", \"p3_median\", \"Final_mean\", \"Final_median\"])\n",
        "\n",
        "p1_avg_mean = p1b_mae_avg_mean\n",
        "p2_avg_mean = p2b_mae_avg_mean\n",
        "p3_avg_mean = p3b_mae_avg_mean\n",
        "\n",
        "p1_avg_median = p1b_mae_avg_median\n",
        "p2_avg_median = p2b_mae_avg_median\n",
        "p3_avg_median = p3b_mae_avg_median\n",
        "\n",
        "f_mean, f_std = get_std(p1_avg_mean, p2_avg_mean, p3_avg_mean)\n",
        "f_median, f_median_std = get_std(p1_avg_median, p2_avg_median, p3_avg_median)\n",
        "\n",
        "table.add_row([num_clusters, round(np.mean(p1_avg_mean), 3), round(np.mean(p2_avg_mean), 3), np.mean(p3_avg_mean),\n",
        "                np.mean(p1_avg_median), np.mean(p2_avg_median), np.mean(p3_avg_median),\n",
        "                f\"{round(f_mean, 3)} ± {round(f_std, 3)}\", f\"{round(f_median, 3)} ± {round(f_median_std, 3)}\"])\n",
        "print(table)\n",
        "\n",
        "print(\"CASE C\")\n",
        "table = PrettyTable([\"cluster\", \"p1_avg_mean\", \"p2_avg_mean\", \"p3_avg_mean\", \"p1_median\", \"p2_median\", \"p3_median\", \"Final_mean\", \"Final_median\"])\n",
        "\n",
        "p1_avg_mean = p1c_mae_avg_mean\n",
        "p2_avg_mean = p2c_mae_avg_mean\n",
        "p3_avg_mean = p3c_mae_avg_mean\n",
        "\n",
        "p1_avg_median = p1c_mae_avg_median\n",
        "p2_avg_median = p2c_mae_avg_median\n",
        "p3_avg_median = p3c_mae_avg_median\n",
        "\n",
        "f_mean, f_std = get_std(p1_avg_mean, p2_avg_mean, p3_avg_mean)\n",
        "f_median, f_median_std = get_std(p1_avg_median, p2_avg_median, p3_avg_median)\n",
        "\n",
        "table.add_row([num_clusters, round(np.mean(p1_avg_mean), 3), round(np.mean(p2_avg_mean), 3), np.mean(p3_avg_mean),\n",
        "                np.mean(p1_avg_median), np.mean(p2_avg_median), np.mean(p3_avg_median),\n",
        "                f\"{round(f_mean, 3)} ± {round(f_std, 3)}\", f\"{round(f_median, 3)} ± {round(f_median_std, 3)}\"])\n",
        "print(table)\n",
        "\n",
        "print(\"CASE D\")\n",
        "table = PrettyTable([\"cluster\", \"p1_avg_mean\", \"p2_avg_mean\", \"p3_avg_mean\", \"p1_median\", \"p2_median\", \"p3_median\", \"Final_mean\", \"Final_median\"])\n",
        "\n",
        "p1_avg_mean = p1d_mae_avg_mean\n",
        "p2_avg_mean = p2d_mae_avg_mean\n",
        "p3_avg_mean = p3d_mae_avg_mean\n",
        "\n",
        "p1_avg_median = p1d_mae_avg_median\n",
        "p2_avg_median = p2d_mae_avg_median\n",
        "p3_avg_median = p3d_mae_avg_median\n",
        "\n",
        "f_mean, f_std = get_std(p1_avg_mean, p2_avg_mean, p3_avg_mean)\n",
        "f_median, f_median_std = get_std(p1_avg_median, p2_avg_median, p3_avg_median)\n",
        "\n",
        "table.add_row([num_clusters, round(np.mean(p1_avg_mean), 3), round(np.mean(p2_avg_mean), 3), np.mean(p3_avg_mean),\n",
        "                np.mean(p1_avg_median), np.mean(p2_avg_median), np.mean(p3_avg_median),\n",
        "                f\"{round(f_mean, 3)} ± {round(f_std, 3)}\", f\"{round(f_median, 3)} ± {round(f_median_std, 3)}\"])\n",
        "print(table)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CASE A\n",
            "+---------+-------------+-------------+--------------------+--------------------+--------------------+---------------------+--------------+---------------+\n",
            "| cluster | p1_avg_mean | p2_avg_mean |    p3_avg_mean     |     p1_median      |     p2_median      |      p3_median      |  Final_mean  |  Final_median |\n",
            "+---------+-------------+-------------+--------------------+--------------------+--------------------+---------------------+--------------+---------------+\n",
            "|    2    |    1.822    |    1.088    | 0.2846814033877334 | 1.3463105835241906 | 0.8047868017841268 | 0.23727714511582476 | 1.065 ± 0.01 | 0.796 ± 0.013 |\n",
            "+---------+-------------+-------------+--------------------+--------------------+--------------------+---------------------+--------------+---------------+\n",
            "CASE B\n",
            "+---------+-------------+-------------+--------------------+--------------------+-------------------+---------------------+---------------+---------------+\n",
            "| cluster | p1_avg_mean | p2_avg_mean |    p3_avg_mean     |     p1_median      |     p2_median     |      p3_median      |   Final_mean  |  Final_median |\n",
            "+---------+-------------+-------------+--------------------+--------------------+-------------------+---------------------+---------------+---------------+\n",
            "|    2    |    3.133    |    1.855    | 0.5510738925359031 | 2.3640963878436585 | 1.368127465666728 | 0.41619359793636557 | 1.847 ± 0.015 | 1.383 ± 0.006 |\n",
            "+---------+-------------+-------------+--------------------+--------------------+-------------------+---------------------+---------------+---------------+\n",
            "CASE C\n",
            "+---------+-------------+-------------+--------------------+-------------------+-------------------+-------------------+---------------+---------------+\n",
            "| cluster | p1_avg_mean | p2_avg_mean |    p3_avg_mean     |     p1_median     |     p2_median     |     p3_median     |   Final_mean  |  Final_median |\n",
            "+---------+-------------+-------------+--------------------+-------------------+-------------------+-------------------+---------------+---------------+\n",
            "|    2    |    6.496    |    8.865    | 10.161369487940087 | 4.576237779052444 | 6.107777895073855 | 6.949637853310246 | 8.508 ± 0.046 | 5.878 ± 0.019 |\n",
            "+---------+-------------+-------------+--------------------+-------------------+-------------------+-------------------+---------------+---------------+\n",
            "CASE D\n",
            "+---------+-------------+-------------+--------------------+------------------+--------------------+-------------------+---------------+---------------+\n",
            "| cluster | p1_avg_mean | p2_avg_mean |    p3_avg_mean     |    p1_median     |     p2_median      |     p3_median     |   Final_mean  |  Final_median |\n",
            "+---------+-------------+-------------+--------------------+------------------+--------------------+-------------------+---------------+---------------+\n",
            "|    2    |    6.854    |    9.246    | 10.266679076885858 | 4.86490070223272 | 6.5566291113271715 | 7.111729093574772 | 8.789 ± 0.044 | 6.178 ± 0.025 |\n",
            "+---------+-------------+-------------+--------------------+------------------+--------------------+-------------------+---------------+---------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OHrdVzhf_6I"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkKvUL2tf_3P"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vU7Y3JoIf_0y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11309aa5-f9d3-4ea1-f42a-d4c72fcbee51"
      },
      "source": [],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    }
  ]
}